[
  {
    "objectID": "posts/2020-11-20-logistic.html",
    "href": "posts/2020-11-20-logistic.html",
    "title": "Logistic Regression’s Coefficients Explanation",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n1. Background\nLogistic regression is used to solve the classification problem. Classification problem means the dependent variable is categorical. For example, we can build a machine learning model to predict whether a student will be admitted to college based on metrics like his or her GPA, standardized test scores. If we formulate a classification problem in mathematical form, we have:\n\\[y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}\\],\nwhere y is a categorical variable, like whether you are admited or not.\nOne way to approach this problem is by using linear regression. However, we would like our algorithm will output a number between 0 and 1, which can indicate the probability of an observation belonging to a certain category. Linear regression does not satisfy this requirement as it might output values smaller than 0 or larger than 1.\nOne solution is to solve this problem is to transform the value of \\(\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}\\) to the range of [0,1]. We can do this with the logistic function: \\[f(x)=\\frac{e^{x}}{1+e^{x}}\\].\nTo show that logistic regression will make a number between 0 and 1. Let’s make a plot of it.\n\ndef logit(x):\n    return np.exp(x)/(1+np.exp(x))\n\n\nx = np.arange(-200, 200, 0.1)\ny = logit(x)\nplt.plot(x,y);\n\n\n\n\nAs we can see, the logit function has a S shape and is bounded by 0 and 1. When x is approaching positive infinity, the logit of x approaches 1. When x is approaching negative infinity, the logit of x approaches 0.\nThus, we can transform the value of \\(\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}\\) to:\n\\[\np(X)=\\frac{e^{\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}}}{1+e^{\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}}}\n\\]\nNow the output of our regression will fall between zero and one. Since we the logistic function to achieve this, the regression is called logistic regression. Before jumping into the details of behind it, let’s first we how to run it in python.\n\n\n2. Python Implementation of Logistic Regression\nFor this post, we are going to use a college admission dataset from UCLA’s Institute for Digital Research & Education. We want to use a student’s GRE score, GPA, and rank to predict whether the student will be admitted.\n\ndf = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      admit\n      gre\n      gpa\n      rank\n    \n  \n  \n    \n      0\n      0\n      380\n      3.61\n      3\n    \n    \n      1\n      1\n      660\n      3.67\n      3\n    \n    \n      2\n      1\n      800\n      4.00\n      1\n    \n    \n      3\n      1\n      640\n      3.19\n      4\n    \n    \n      4\n      0\n      520\n      2.93\n      4\n    \n  \n\n\n\n\nWe are going to use the statsmodel package to run the logistic regression. We choose statsmodel because its interface and summary of the result are more similar to other statistical software, like R and Stata.\n\nimport statsmodels.formula.api as smf\n\n\n#collapse_output\nmod = smf.logit(formula='admit ~ gre + gpa + rank', data=df).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.574302\n         Iterations 6\n\n\n\nprint(mod.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  admit   No. Observations:                  400\nModel:                          Logit   Df Residuals:                      396\nMethod:                           MLE   Df Model:                            3\nDate:                Tue, 02 Feb 2021   Pseudo R-squ.:                 0.08107\nTime:                        09:16:16   Log-Likelihood:                -229.72\nconverged:                       True   LL-Null:                       -249.99\nCovariance Type:            nonrobust   LLR p-value:                 8.207e-09\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.4495      1.133     -3.045      0.002      -5.670      -1.229\ngre            0.0023      0.001      2.101      0.036       0.000       0.004\ngpa            0.7770      0.327      2.373      0.018       0.135       1.419\nrank          -0.5600      0.127     -4.405      0.000      -0.809      -0.311\n==============================================================================\n\n\n\n\n3. Interpretation of the Result\n\n3.1 Prediction\nLet’s make a prediction for a student with GRE 300, GPA 2, and rank 4.\n\n#the 1 is the constant term for intercept\na_new_student = np.array([1, 300, 2, 4])\n\nWe extract the parameters from the logistic regression model to make the prediction.\n\nmodel_parameters = mod.params.values\nmodel_parameters\n\narray([-3.44954840e+00,  2.29395950e-03,  7.77013574e-01, -5.60031387e-01])\n\n\nNote that the following calculation can be made much easier with matrix multiplication with code like a_new_student @ model_parameters.\n\npred = 0\nfor i in range(4):\n    pred += model_parameters[i] * a_new_student[i]\npred\n\n-3.4474589462957206\n\n\nNote that the result is negative. Did we say in the previous section the logistic regression guarantees a prediction between 0 and 1? Then why do we get this negative prediction? To understand this, we need to understand the relationship between three terms - probability, odd, and log-odd.\n\n\n3.2 Probability, odd, log-odd\nLet \\(k = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3}\\). As we discussed in section 1, we have: \\(p(X)=\\frac{e^{k}}{1+e^{k}}\\), which is the probability of X belonging to a certain class.\nThen, \\(p(x) + p(x)e^k = e^k\\). Then \\(p(x) = e^k - p(x)e^k = e^k(1-p(x))\\). Thus, \\(e^{k} = \\frac{p(x)}{1-p(x)}\\). The quantity \\(\\frac{p(x)}{1-p(x)}\\) is called the odds.\nTaking log from both side the odds equation, we have \\(k= log(\\frac{p(x)}{1-p(x)})\\). This quantity \\(log(\\frac{p(x)}{1-p(x)})\\) the log-odds.\nBringing back the value of k, we have: \\(log(\\frac{p(x)}{1-p(x)}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3}\\).\nWhat we are actually evaluating when running the python code in the section is the equation above. Thus, the prediction we made is not the probability, but the log-odds.\nTo evaluate the probability, we can put the log-odds we predict back to the formula \\(p(X)=\\frac{e^{k}}{1+e^{k}}\\), as k is the log-odd we predict.\nIn code, we have:\n\ndef prob_from_log_odd(log_odds):\n    prob = np.exp(log_odds)/(1+np.exp(log_odds))\n    return prob\n\nUsing the prediction we have in section 3.1, we have:\n\nprint(\"The log odd is: \", pred)\nprint(\"The probability is: \", prob_from_log_odd(pred))\n\nThe log odd is:  -3.4474589462957206\nThe probability is:  0.03084472943016926\n\n\nNote that the probability is about 0.03, which is between 0 and 1. To determine whether the student will be admitted or not, we usually set a 0.5 as the threshold. For students with predicted probability lower than 0.5, we will predict the result as reject. Otherwise, we will predict the result as admit.\n\n\n3.3 Coefficient Explanation\nBased on the discussion in section 3.2, the explanation of a specific coefficient of the logistic regression is that given all other variables being the same, how much log-odd will change given one unit increase of the independent variable. For example, the coefficient for gre is about 0.0023, which can be interpreted as given all other variables being the same, one additional GRE score will lead to a 0.0023 increase of the log odd.\n\n\n\n4. Estimation Method\nOne question we did not answer so far is that how do we get the estimation of these coefficients for logistic regression. The estimation uses a statistical method called maximum likelihood estimation. The details for this method is beyond the scope of this post. The book The Elements of Statistical Learning gives a detailed discussion of the estimation process in section 4.4. A Python implementation of the estimation from scratch can be found in this post."
  },
  {
    "objectID": "posts/2022-01-26-pydantic.html",
    "href": "posts/2022-01-26-pydantic.html",
    "title": "My Understanding of Pydantic",
    "section": "",
    "text": "I have heard about pydantic for a long time since its usage in popular libraries like FastAPI. However, I am still confused about the functionality of this library when reading the introduction of documentation. This book helps me understand this topic a lot better.\nIn short, pydantic can help us validate (or parse) data during the runtime with type checks and avoid using a large amount of if-else conditions to validate the data. You might be as confused as me when you read this sentence for the first time. Let’s walk through an example together.\nSuppose we have a deep learning model training script. One of the function inside it requires the user to pass the learning rate and optimizer in a yaml file. You can learn more about yaml file here if you are not familiar with it.\nThe yaml file might look like the following.\nmodel_config:\n  learning_rate: 0.01\n  optimizer: adam\nA sample function can look like the following:\nfrom typing import Dict\n\nimport yaml\n\n\nwith open('config.yaml') as yaml_file:\n    config: Dict = yaml.safe_load(yaml_file)\nmodel_config = config[\"model_config\"]\n\ndef train_model(model_config: Dict):\n    lr = model_config[\"learning_rate\"]\n    optimizer = config_file[\"optmizer\"]\n    # the model training code ...\nHowever, we want to restrict the learning rate to be a float between 0.0 and 1.0, and force optimizer to be one of the following: sgd, rmsprop, adam. Instead of throwing the error when the model begins to train or letting the model fail silently (the code runs but the model cannot converge because of a huge learning rate), we want to catch this error when we load the data from yaml.\nOne naive solution is to use try-except and if-else statements like the following.\ntry:\n    lr = float(model_config[\"learning_rate\"])\nexcept:\n    raise ValueError(\"learning rate needs to be a float\")\n\noptimizer = str(model_config[\"optimizer\"])\n\nif lr < 0. or lr > 1.:\n    raise ValueError(\"learning rate needs to between 0.0 and 1.0\")\n\nif optimizer not in [\"sgd\", \"rmsprop\", \"adam\"]:\n    raise ValueError(\"optimizer needs to be sgd, rmsprop, or adam\")\nThis can get extremely long and complex when have a large list of hyper parameters. Also, even we want to use type hint and type checker like mypy to get rid of the checking for type like the following (Note that you need to have Python 3.8 or above to use the Literal type).\nfrom typing import Literal, TypedDict\n\nclass ModelConfig(TypedDict):\n    learning_rate: float \n    optimizer: Literal[\"sgd\", \"rmsprop\", \"adam\"]\n \ndef load_yaml(file_path: str) -> ModelConfig:\n    with open(file_path) as yaml_file:\n      config = yaml.safe_load(yaml_file)\n    return config[\"model_config\"]\nWe are still out of luck because we don’t know the information in the yaml file until we actually run the script (during runtime). Thus, even we have a yaml file like this:\nmodel_config:\n  learning_rate: not_learning\n  optimizer: 1.0\nType checker like mypy will not throw any error. This is when pydantic comes in. It checks type during the runtime.\nfrom pydantic import BaseModel, confloat\n\nclass ModelConfig(BaseModel):\n    learning_rate: confloat(gt=0., lt=1.)\n    optimizer: Literal[\"sgd\", \"rmsprop\", \"adam\"]\n\ndef load_yaml(file_path: str) -> ModelConfig:\n    with open(file_path) as yaml_file:\n        config = yaml.safe_load(yaml_file)\n    return ModelConfig(**config[\"model_config\"])\nLet’s explain few things in this code.\n\nWe inherit from the BaseModel class of the pydantic model so when we construct this class during the run time, the type checking will be executed.\nThe confloat type in pydantic allows us to restrict the range of a float value. In our case, we require it to be greater than 0 or less than 1.\nWe use double asterisk (**) to construct the class from a dictionary. You can learn more about it here.\n\nNow, if we run the script by passing it the wrong yaml config file above, we will get the following error.\nValidationError: 2 validation errors for ModelConfig\nlearning_rate\n  value is not a valid float (type=type_error.float)\noptimizer\n  unexpected value; permitted: 'sgd', 'rmsprop', 'adam' (type=value_error.const; given=1.0; permitted=('sgd', 'rmsprop', 'adam'))\nNote that pydantic does the validation for us automatically and simplifies our code quite a bit. To reiterate, pydantic can help us validate (or parse) data during the runtime with type checks and avoid using a large amount of if-else conditions to validate the data. Hopefully now this sentence makes more sense to you.\nMy understanding of pydantic is still at a basic level and I encourage you to check out the documentation of pydantic to learn more about it. Also, the robust python is a really nice book for introducing typing in Python and many other good coding practices."
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html",
    "href": "posts/2020-12-21-fastai_emb.html",
    "title": "Entity Embedding in fastai 2",
    "section": "",
    "text": "This post aims to introduce how to use fastai v2 to implement entity embedding for categorical variables in tabular data. Entity embedding is a powerful technique that can sometimes boost the performance of various machine learning methods and reveal the intrinsic properties of categorical variables. See this paper and this post for more details."
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#get-data",
    "href": "posts/2020-12-21-fastai_emb.html#get-data",
    "title": "Entity Embedding in fastai 2",
    "section": "Get Data",
    "text": "Get Data\nWe will use the California housing dataset for this post. We want to develop a model to predict the median_house_value based on other variables in the dataset. The following code shows the first 5 rows of the dataset.\n\ndf = pd.read_csv('housing.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n  \n    \n      0\n      -122.23\n      37.88\n      41.0\n      880.0\n      129.0\n      322.0\n      126.0\n      8.3252\n      452600.0\n      NEAR BAY\n    \n    \n      1\n      -122.22\n      37.86\n      21.0\n      7099.0\n      1106.0\n      2401.0\n      1138.0\n      8.3014\n      358500.0\n      NEAR BAY\n    \n    \n      2\n      -122.24\n      37.85\n      52.0\n      1467.0\n      190.0\n      496.0\n      177.0\n      7.2574\n      352100.0\n      NEAR BAY\n    \n    \n      3\n      -122.25\n      37.85\n      52.0\n      1274.0\n      235.0\n      558.0\n      219.0\n      5.6431\n      341300.0\n      NEAR BAY\n    \n    \n      4\n      -122.25\n      37.85\n      52.0\n      1627.0\n      280.0\n      565.0\n      259.0\n      3.8462\n      342200.0\n      NEAR BAY"
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#preprocess-data",
    "href": "posts/2020-12-21-fastai_emb.html#preprocess-data",
    "title": "Entity Embedding in fastai 2",
    "section": "Preprocess Data",
    "text": "Preprocess Data\nWe rely on various functionalities provided by fastai to preprocess the data. We will not discuss them in detail here. For more information, please check this tutorial. One important note is that when dealing with categorical variables, instead of using one-hot encoding, we map each category into a distinct integer.\n\ncont, cat = cont_cat_split(df, dep_var = 'median_house_value')\n\n\nsplits = RandomSplitter(valid_pct=0.2)(range_of(df))\n\n\nto = TabularPandas(df, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = cat,\n                   cont_names = cont,\n                   y_names='median_house_value',\n                   splits=splits)\n\n\ndls = to.dataloaders(bs=64)"
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#train-model",
    "href": "posts/2020-12-21-fastai_emb.html#train-model",
    "title": "Entity Embedding in fastai 2",
    "section": "Train Model",
    "text": "Train Model\nWe will train a deep learning model to predict the median housing value and thus get the trained embedding for categorical variables. Again, see this tutorial for details about the meaning of the codes here.\n\nlearn = tabular_learner(dls, metrics=rmse)\n\n\nearly_stop_cb = EarlyStoppingCallback(patience=2)\n\n\n#collapse-output\nlearn.fit_one_cycle(10, cbs=early_stop_cb)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      55994937344.000000\n      56636186624.000000\n      237983.578125\n      00:04\n    \n    \n      1\n      56567816192.000000\n      56058937344.000000\n      236767.671875\n      00:04\n    \n    \n      2\n      52564221952.000000\n      52214001664.000000\n      228503.843750\n      00:03\n    \n    \n      3\n      45972250624.000000\n      46276349952.000000\n      215119.375000\n      00:03\n    \n    \n      4\n      37385846784.000000\n      37008117760.000000\n      192374.937500\n      00:03\n    \n    \n      5\n      31154010112.000000\n      30351321088.000000\n      174216.312500\n      00:03\n    \n    \n      6\n      24364992512.000000\n      23757260800.000000\n      154133.906250\n      00:03\n    \n    \n      7\n      22229555200.000000\n      21549965312.000000\n      146799.031250\n      00:03\n    \n    \n      8\n      20496922624.000000\n      21110308864.000000\n      145293.875000\n      00:03\n    \n    \n      9\n      20571899904.000000\n      20511488000.000000\n      143218.312500\n      00:03"
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#get-embedding",
    "href": "posts/2020-12-21-fastai_emb.html#get-embedding",
    "title": "Entity Embedding in fastai 2",
    "section": "Get Embedding",
    "text": "Get Embedding\nWe will retrieve the trained embedding matrix from the model.\n\nembs = [param for param in learn.model.embeds.parameters()]\n\nThe list has two elements and each element represents an embedding matrix for a categorical variable.\n\nlen(embs)\n\n2\n\n\nTo check what each element corresponds to, we can use the cat_names attributes from TabularPandas. The list indicates that the first element in embs is the embedding matrix for the variable ocean_proximity.\n\nto.cat_names\n\n(#2) ['ocean_proximity','total_bedrooms_na']\n\n\nLet’s see this matrix. Note that we convert it from a tensor array to a numpy array to make the operation later easier.\n\nocean_emb = embs[0].detach().numpy()\nocean_emb\n\narray([[ 0.00074246,  0.01322438, -0.00539126,  0.00066858],\n       [-0.01550745, -0.02467075,  0.01956671, -0.01148492],\n       [ 0.24266721,  0.35117084, -0.35875005,  0.36369336],\n       [-0.05629548, -0.06088502,  0.08677642, -0.10355063],\n       [-0.01115109, -0.02015582,  0.01277605, -0.00476734],\n       [-0.01796132, -0.02396758,  0.02242134, -0.0143294 ]],\n      dtype=float32)\n\n\nEach row in the matrix above corresponds to one category in the categorical variable. The categories for ocean_proximity are the following.\n\ncat = to.procs.categorify\nocean_cat = cat['ocean_proximity']\nocean_cat\n\n['#na#', '<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n\n\nWe can use the o2i attribute to see how each category is mapped into an integer. The integer corresponds to the row in the embedding matrix.\n\nocean_cat.o2i\n\ndefaultdict(int,\n            {'#na#': 0,\n             '<1H OCEAN': 1,\n             'INLAND': 2,\n             'ISLAND': 3,\n             'NEAR BAY': 4,\n             'NEAR OCEAN': 5})\n\n\nFor example, the embedding for ‘NEAR BAY’ is:\n\nocean_emb[4]\n\narray([-0.01115109, -0.02015582,  0.01277605, -0.00476734], dtype=float32)\n\n\nLet’s create a dictionary to map each category to its corresponding embedding.\n\nocean_emb_map = {ocean_cat[i]:ocean_emb[i] for i in range(len(ocean_cat))}"
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#apply-embedding",
    "href": "posts/2020-12-21-fastai_emb.html#apply-embedding",
    "title": "Entity Embedding in fastai 2",
    "section": "Apply Embedding",
    "text": "Apply Embedding\nThis section shows how we can apply the embedding to the original dataset; that is, change the category variable to numeric vectors.\n\nemb_dim = ocean_emb.shape[1]\n\n\ncol_name = [f'ocean_emb_{i}' for i in range(1,emb_dim+1)]\ndf_emb = pd.DataFrame(df['ocean_proximity'].map(ocean_emb_map).to_list(), columns=col_name)\ndf_emb.head()\n\n\n\n\n\n  \n    \n      \n      ocean_emb_1\n      ocean_emb_2\n      ocean_emb_3\n      ocean_emb_4\n    \n  \n  \n    \n      0\n      -0.011151\n      -0.020156\n      0.012776\n      -0.004767\n    \n    \n      1\n      -0.011151\n      -0.020156\n      0.012776\n      -0.004767\n    \n    \n      2\n      -0.011151\n      -0.020156\n      0.012776\n      -0.004767\n    \n    \n      3\n      -0.011151\n      -0.020156\n      0.012776\n      -0.004767\n    \n    \n      4\n      -0.011151\n      -0.020156\n      0.012776\n      -0.004767\n    \n  \n\n\n\n\n\ndf_new = pd.concat([df, df_emb],axis=1)"
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#visualize-embedding",
    "href": "posts/2020-12-21-fastai_emb.html#visualize-embedding",
    "title": "Entity Embedding in fastai 2",
    "section": "Visualize Embedding",
    "text": "Visualize Embedding\nAnother way to explore the embedding matrix is to visualize it and see what it learns. We will use principle component analysis(PCA) to visualize the embedding.\n\nfrom sklearn import decomposition\npca = decomposition.PCA(n_components=2)\npca_result = pca.fit_transform(ocean_emb)\n\n\ndf_visualize = pd.DataFrame({'name':ocean_cat, 'dim1':pca_result[:,0], 'dim2':\n                             pca_result[:,1]})\nsns.scatterplot(data=df_visualize, x='dim1', y='dim2', hue='name');\n\n\n\n\nIf we compare this visualization to the map of these houses above, we will find that the embedding matrix does provide some useful insights. In particular, we see that the relative location of the category INLAND does correspond to the inland area in the map. The code to make the following map comes from this notebook."
  },
  {
    "objectID": "posts/2020-12-21-fastai_emb.html#ending-note",
    "href": "posts/2020-12-21-fastai_emb.html#ending-note",
    "title": "Entity Embedding in fastai 2",
    "section": "Ending Note",
    "text": "Ending Note\nThis is the end of this post. You can see the full code by clicking to the View on Github or Open in Colab tab at the top this post."
  },
  {
    "objectID": "posts/2021-08-19-ds_cmd.html",
    "href": "posts/2021-08-19-ds_cmd.html",
    "title": "Notes from Data Science at the Command Line",
    "section": "",
    "text": "In this post, I want to write down some ideas from Jeroen Janssens’s book Data Science at the Command Line, 2e. Instead of being a comprehensive overview of the book, this note only picks some ideas that add to my existing knowledge and hopefully it can bring some new thoughts to you. Of course, this blog post reflects my personal understanding and all errors remain mine."
  },
  {
    "objectID": "posts/2021-08-19-ds_cmd.html#introduction",
    "href": "posts/2021-08-19-ds_cmd.html#introduction",
    "title": "Notes from Data Science at the Command Line",
    "section": "Introduction",
    "text": "Introduction\nThe original book can be divided into two parts. The first part covers how to use the command line to do four essential parts of data science: (1) obtaining data, (2) scrubbing data, (3) exploring data, (4) modeling data. The second part of the book introduces some useful command-line tools and methods for doing data science, including: (1) create command-line tools with shell script, Python and R; (2) use Makefile to do project and pipeline management; (3) implement parallel computing in the command line; (4) incorporate command line tools into other programming languages, including Python, R and Spark.\nHowever, to summarize my learning, I will break this blog post into four parts:\n\nUseful tools: some command line packages and tools, including Docker, Make, and few other packages.\nSyntax tricks: some useful syntax that can be helpful for daily work.\nIntegration with Python: how to combine the power of command line tools with Python.\nUseful Resources: some interesting resources for further learning."
  },
  {
    "objectID": "posts/2021-08-19-ds_cmd.html#useful-tools",
    "href": "posts/2021-08-19-ds_cmd.html#useful-tools",
    "title": "Notes from Data Science at the Command Line",
    "section": "Useful Tools",
    "text": "Useful Tools\n\nDocker\nDocker in itself is a fascinating but complex topic and thus is beyond the scope of this blog and the original book. However, one idea I learn from the book is how the author uses Docker as an educational tool, which allows users to set up the same environment and follow the book easily. After installing Docker, just running two lines of code in the command line will set up the prepared environment from the book.\ndocker pull datasciencetoolbox/dsatcl2e\ndocker run --rm -it datasciencetoolbox/dsatcl2e\nAnother trick about Docker I learned is how the author uses the -v flag to get data in and out of container with a command like the following. (In macOS or Linux).\ndocker run --rm -it -v \"$(pwd)\":/data datasciencetoolbox/dsatcl2e\nThe author recommended another book called Docker: Up & Running to learn more about Docker. You can also see this A Docker Tutorial for Beginners to learn more about Docker.\n\n\nMake\nI was fortunate enough to learn and use make in the past, which is one of the most valuable tools I learned in 2021. If you haven’t learned, you should check out chapter 6 of Data Science at the Command Line, 2e to learn about it. make provides a nice way to build a data science pipeline and simplify complex commands.\nI will only highlight few tips I learn from chapter 6 here.\n\nIf the configuration file is not called Makefile (usually you shouldn’t do this), you can run still run the file with a -f flag. For example, if you name the configuration file hello.make, I can run it with the following:\nmake -f hello.make\nYou can use the automatic variable $@ to avoid some duplicates. For example, the following two commands will be equivalent.\nnumbers:\n seq 7 > $@\n\nnumbers:\n seq 7 > numbers\nWe can change the configuration of the make with the following codes in the top of the Makefile\nSHELL := bash\n.SHELLFLAGS := -eu -o pipefail -c\nI will directly quote the author to explain this:\n\n\nAll rules are executed in a shell, which by default, is sh. With the SHELL variable we can change this to another shell, like bash. This way we can use everything that Bash has to offer such as for loops.\nThe .SHELLFLAGS line makes Bash more strict, which is considered a best practice. For example, because of this, the pipeline in the rule for target top10 now stops as soon as there is an error.\n\n\n\n\n\nOther tools\nThere are two command line packages introduced in the book I particularly like: bat and tldr. Note that usually these two tools are not pre-installed and thus you need to install them with instructions from the link.\nbat is an enhanced version of the common command-line tool cat. The following picture shows three benefits from bat:\n\nIt provides syntax highlighting.\nIt shows modification information from git (see the red line in line 10).\nWhen showing a long file, instead of overwhelming the terminal with all the information like cat, it pipes the output to a pager like the less command.\n\n\ntldr aims to simplify the man command manual with concrete examples. Consider the following two manuals for the find command.\n\n\nIf you are not looking for some advanced and specific usages, the output page from tldr provides simpler and more useful instructions for understanding a command."
  },
  {
    "objectID": "posts/2021-08-19-ds_cmd.html#syntax-trick",
    "href": "posts/2021-08-19-ds_cmd.html#syntax-trick",
    "title": "Notes from Data Science at the Command Line",
    "section": "Syntax Trick",
    "text": "Syntax Trick\nThere are several useful syntaxes I learn from the book.\n\nThe following command prints the variable in the variable PATH into separate lines, which can be helpful to debug issues around the environment path.\necho $PATH | tr ':' '\\n'\nShebang\n\nFrom the book: “The shebang is a special line in the script that instructs the system which executable it should use to interpret the commands.”\nYou can see an example of shebang below. (#!/usr/bin/env python in the Python script).\nThe author recommends using commands like !/usr/bin/env bash instead of !/usr/bin/bash because this can avoid errors when executables are not in /usr/bin."
  },
  {
    "objectID": "posts/2021-08-19-ds_cmd.html#integration-with-python",
    "href": "posts/2021-08-19-ds_cmd.html#integration-with-python",
    "title": "Notes from Data Science at the Command Line",
    "section": "Integration with Python",
    "text": "Integration with Python\nAnother useful thing I learned is to combine shell commands with Python. Specifically, there are two ways: (1) using shell command in Jupyter; (2) using Python to write shell scripts.\n\nShell Command in Jupyter\nIn a Python code shell of jupyter notebook or jupyter lab, we can call use shell commands by adding ! before the command. For example, we can list the file in the current directory with\n! ls\nOr even assign the output of the command line to a Python variable. For example, we can do:\nfile_list = ! ls\nThis functionality allows us to utilize the brevity of shell commands to achieve things quickly. One example the author gives in the book is about downloading a file from the Internet.\nurl = \"https://www.gutenberg.org/files/11/old/11.txt\"\n\n# Command line way (curly braces allow us to add python variable)\n! curl '{url}' > alice3.txt\n\n# Python way\nimport requests \n\nwith open(\"alice2.txt\", \"wb\") as f:\n  response = requests.get(url)\n  f.write(response.content)\nAs we can see, the command line achieves the same thing with less code but still being easy to understand.\nAnother syntax trick is that if you want to use literal curly braces for command line, you can type it twice in jupyter like the following:\n{% raw %}\n! rm -v alice{{2,3}}.txt\n{% endraw %}\n\n\nUse Python to Write Shell Scripts\nWe can create a shell script with Python and combine it into a command line pipeline. One example the author gives in the book is a script for the Fizz Buzz problem. The script is called fizzbuzz.py.\n#!/usr/bin/env python\nimport sys\n\nCYCLE_OF_15 = [\"fizzbuzz\", None, None, \"fizz\", None,\n               \"buzz\", \"fizz\", None, None, \"fizz\",\n               \"buzz\", None, \"fizz\", None, None]\n\n## See the usage of type hint with :int and -> str\ndef fizz_buzz(n: int) -> str:\n    ## In Python, (None or 1) = 1 ('buzz' or 5) = 'buzz'\n    ## if the first element in or is evaluated as \n    ## None or False, it will go to the second element\n    return CYCLE_OF_15[n % 15] or str(n)\n\nif __name__ == \"__main__\":\n    try:\n        ## See the walrus operator := below\n        ## which allows assign and evaluate at the same time\n        while (n:= sys.stdin.readline()):\n            print(fizz_buzz(int(n)))\n    except:\n        pass\nWith this script, you can combine it into a command line pipeline with the following command, which will show the result of Fizz Buzz problem for number from 1 to 30.\nseq 30 | ./fizzbuzz.py | column -x\nAnother thing I like is the way the author writes the Python code. In the simple script above, we can learn several Python techniques, including the walrus operator introduced in Python 3.8, type hint for functions, and the usage of or. I wrote my own comment with ## in the command to annotate them."
  },
  {
    "objectID": "posts/2021-08-19-ds_cmd.html#useful-resources",
    "href": "posts/2021-08-19-ds_cmd.html#useful-resources",
    "title": "Notes from Data Science at the Command Line",
    "section": "Useful Resources",
    "text": "Useful Resources\nThe first resource is definitely to read through the book Data Science at the Command Line, 2e, which has a free online version. This blog only reflects my personal learning. Given everyone’s unique background, you are likely to learn something different from the book. In addition to that, the author also recommends several resources inside the book which I find interesting:\n\nThinking with Data: this book seems to be a friendly introduction to how to approach real-world problems with a data-driven approach\nExplain Shell: a useful website that parses a command in the shell and explain the usage of each part\nTen Essays on Fizz Buzz: a book that discusses very aspects of Python and software design more general through 10 solutions for the Fizz Buzz problem\nPro Git: a comprehensive guide to git\nRegular Expressions Cookbook: a guide for regular expressions\n\nI hadn’t read any of the books above in detail but plan to do so and will try to share my notes for them once I finished."
  },
  {
    "objectID": "posts/2021-11-18-roc.html",
    "href": "posts/2021-11-18-roc.html",
    "title": "Construct ROC Curve from Scratch",
    "section": "",
    "text": "This post aims to show how to construct the receiver operating characteristic (roc) curve without using predefined functions. It hopes to help you better understand how the roc curve is constructed and how to interpret it."
  },
  {
    "objectID": "posts/2021-11-18-roc.html#setup",
    "href": "posts/2021-11-18-roc.html#setup",
    "title": "Construct ROC Curve from Scratch",
    "section": "Setup",
    "text": "Setup\n\n\nCode\n%matplotlib inline\n\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/2021-11-18-roc.html#load-data-and-train-model",
    "href": "posts/2021-11-18-roc.html#load-data-and-train-model",
    "title": "Construct ROC Curve from Scratch",
    "section": "Load Data and Train Model",
    "text": "Load Data and Train Model\nThe dataset we will use for this blog is the famous Titanic dataset. For simplicity, we are going to use a subset of the data that does not contain missing values.\n\n\nCode\ndata_path = \"https://raw.githubusercontent.com/wpan03/quick_ds_python/master/data/titanic_train.csv\"\nsample_size = 200\nseed = 36\n\n\n\n\nCode\ndf_titanic = (pd.read_csv(data_path)\n                .dropna(subset=['Age'])\n                .sample(sample_size, random_state=seed)\n                .reset_index(drop=True)\n             )\n\n\nThis is the first 5 rows of the dataset.\n\ndf_titanic.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      690\n      1\n      1\n      Madill, Miss. Georgette Alexandra\n      female\n      15.0\n      0\n      1\n      24160\n      211.3375\n      B5\n      S\n    \n    \n      1\n      272\n      1\n      3\n      Tornquist, Mr. William Henry\n      male\n      25.0\n      0\n      0\n      LINE\n      0.0000\n      NaN\n      S\n    \n    \n      2\n      52\n      0\n      3\n      Nosworthy, Mr. Richard Cater\n      male\n      21.0\n      0\n      0\n      A/4. 39886\n      7.8000\n      NaN\n      S\n    \n    \n      3\n      516\n      0\n      1\n      Walker, Mr. William Anderson\n      male\n      47.0\n      0\n      0\n      36967\n      34.0208\n      D46\n      S\n    \n    \n      4\n      666\n      0\n      2\n      Hickman, Mr. Lewis\n      male\n      32.0\n      2\n      0\n      S.O.C. 14879\n      73.5000\n      NaN\n      S\n    \n  \n\n\n\n\nWe train a simple logistic regression model by using only two columns as the independent variables - Fare and Age and try to predict whether a passenger can survive in the accident.\n\nx_data = df_titanic[[\"Fare\", \"Age\"]]\ny = df_titanic[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(\n    x_data, y, test_size=0.2, random_state=seed\n)\n\n\nmod_lg = LogisticRegression(random_state=seed).fit(x_train, y_train)"
  },
  {
    "objectID": "posts/2021-11-18-roc.html#get-roc-curve",
    "href": "posts/2021-11-18-roc.html#get-roc-curve",
    "title": "Construct ROC Curve from Scratch",
    "section": "Get ROC Curve",
    "text": "Get ROC Curve\nWith the model setup, we can go into the core steps for constructing the roc curve. Constructing the roc curve includes 4 steps (this is adapted from lecture notes from Professor Spenkuch’s business analytics class).\n\nSort predicted probability of “positive” outcome for each observation.\nFor each observation, record false positive rate (fpr) and true positive rate (tpr) if that observation’s predicted probability were used as classification threshold.\nPlot recorded pairs of tpr and fpr.\nConnect the dots.\n\nLet’s show how to do those step by step.\nFirst, we can get the sorted probability of positive outcomes (prediction == 1) of the next two lines of code.\n\ny_pred_proba = mod_lg.predict_proba(x_test)[:, 1]\ny_pred_proba_asc = np.sort(y_pred_proba)\n\nSecond, we define a function to calculate the fpr and tpr for a given threshold.\n\ndef get_tpr_fpr_pair(\n    y_proba: np.ndarray, y_true: Union[np.ndarray, pd.Series], threshold: float\n) -> Tuple[float, float]:\n    \"\"\"Get the true positive rate and false positive rate based on a certain threshold\"\"\"\n    y_pred = (y_proba >= threshold).astype(int)\n    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n    tpr = tp / (tp + fn)\n    fpr = fp / (fp + tn)\n    return tpr, fpr\n\nWith the function defined above, we can loop through each element in the sorted probability of positive outcomes and use each element as the threshold for our get_tpr_fpr_pair function and store the result in two lists.\n\ntpr_list = []\nfpr_list = []\nfor t in y_pred_proba_asc:\n    tpr, fpr = get_tpr_fpr_pair(y_pred_proba, y_test, threshold=t)\n    tpr_list.append(tpr)\n    fpr_list.append(fpr)\n\nFinally, after we have the record for each pair of tpr and fpr, we can plot them to get the roc curve.\n\nplt.scatter(fpr_list, tpr_list)\nplt.plot(fpr_list, tpr_list)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n\n\n\n\nThe following code shows what we construct from scratch is the same as what we get from the predefined functions in scikit-learn.\n\nmetrics.RocCurveDisplay.from_estimator(mod_lg, x_test, y_test)\nplt.show()\n\n\n\n\nThat’s it for this blog post. To learn ROC and AUC from another perspective, you can check out this excellent video here."
  },
  {
    "objectID": "posts/2021-11-18-roc.html#appendix",
    "href": "posts/2021-11-18-roc.html#appendix",
    "title": "Construct ROC Curve from Scratch",
    "section": "Appendix",
    "text": "Appendix\nThis appendix shows the Python and library version we used when writing this blog.\n\n%load_ext watermark\n%watermark --iversions\n\nsys       : 3.8.8 (default, Feb 24 2021, 13:46:16) \n[Clang 10.0.0 ]\npandas    : 1.3.1\nnumpy     : 1.19.2\nmatplotlib: 3.3.2\nsklearn   : 1.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Journal of Data Science Journey",
    "section": "",
    "text": "Jan 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2021\n\n\nMartin Pan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2020\n\n\nMartin Pan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2020\n\n\nMartin Pan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]